// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow_serving/apis/inference.proto

#include "tensorflow_serving/apis/inference.pb.h"

#include <algorithm>

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/extension_set.h>
#include <google/protobuf/wire_format_lite.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>

PROTOBUF_PRAGMA_INIT_SEG

namespace _pb = ::PROTOBUF_NAMESPACE_ID;
namespace _pbi = _pb::internal;

namespace tensorflow {
namespace serving {
PROTOBUF_CONSTEXPR InferenceTask::InferenceTask(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.method_name_)*/{&::_pbi::fixed_address_empty_string, ::_pbi::ConstantInitialized{}}
  , /*decltype(_impl_.model_spec_)*/nullptr
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct InferenceTaskDefaultTypeInternal {
  PROTOBUF_CONSTEXPR InferenceTaskDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~InferenceTaskDefaultTypeInternal() {}
  union {
    InferenceTask _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 InferenceTaskDefaultTypeInternal _InferenceTask_default_instance_;
PROTOBUF_CONSTEXPR InferenceResult::InferenceResult(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.model_spec_)*/nullptr
  , /*decltype(_impl_.result_)*/{}
  , /*decltype(_impl_._cached_size_)*/{}
  , /*decltype(_impl_._oneof_case_)*/{}} {}
struct InferenceResultDefaultTypeInternal {
  PROTOBUF_CONSTEXPR InferenceResultDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~InferenceResultDefaultTypeInternal() {}
  union {
    InferenceResult _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 InferenceResultDefaultTypeInternal _InferenceResult_default_instance_;
PROTOBUF_CONSTEXPR MultiInferenceRequest::MultiInferenceRequest(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.tasks_)*/{}
  , /*decltype(_impl_.input_)*/nullptr
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct MultiInferenceRequestDefaultTypeInternal {
  PROTOBUF_CONSTEXPR MultiInferenceRequestDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~MultiInferenceRequestDefaultTypeInternal() {}
  union {
    MultiInferenceRequest _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 MultiInferenceRequestDefaultTypeInternal _MultiInferenceRequest_default_instance_;
PROTOBUF_CONSTEXPR MultiInferenceResponse::MultiInferenceResponse(
    ::_pbi::ConstantInitialized): _impl_{
    /*decltype(_impl_.results_)*/{}
  , /*decltype(_impl_._cached_size_)*/{}} {}
struct MultiInferenceResponseDefaultTypeInternal {
  PROTOBUF_CONSTEXPR MultiInferenceResponseDefaultTypeInternal()
      : _instance(::_pbi::ConstantInitialized{}) {}
  ~MultiInferenceResponseDefaultTypeInternal() {}
  union {
    MultiInferenceResponse _instance;
  };
};
PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 MultiInferenceResponseDefaultTypeInternal _MultiInferenceResponse_default_instance_;
}  // namespace serving
}  // namespace tensorflow
static ::_pb::Metadata file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto[4];
static constexpr ::_pb::EnumDescriptor const** file_level_enum_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto = nullptr;
static constexpr ::_pb::ServiceDescriptor const** file_level_service_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto = nullptr;

const uint32_t TableStruct_tensorflow_5fserving_2fapis_2finference_2eproto::offsets[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceTask, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceTask, _impl_.model_spec_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceTask, _impl_.method_name_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _internal_metadata_),
  ~0u,  // no _extensions_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _impl_._oneof_case_[0]),
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _impl_.model_spec_),
  ::_pbi::kInvalidFieldOffsetTag,
  ::_pbi::kInvalidFieldOffsetTag,
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _impl_.result_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceRequest, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceRequest, _impl_.tasks_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceRequest, _impl_.input_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceResponse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  ~0u,  // no _inlined_string_donated_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceResponse, _impl_.results_),
};
static const ::_pbi::MigrationSchema schemas[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  { 0, -1, -1, sizeof(::tensorflow::serving::InferenceTask)},
  { 8, -1, -1, sizeof(::tensorflow::serving::InferenceResult)},
  { 18, -1, -1, sizeof(::tensorflow::serving::MultiInferenceRequest)},
  { 26, -1, -1, sizeof(::tensorflow::serving::MultiInferenceResponse)},
};

static const ::_pb::Message* const file_default_instances[] = {
  &::tensorflow::serving::_InferenceTask_default_instance_._instance,
  &::tensorflow::serving::_InferenceResult_default_instance_._instance,
  &::tensorflow::serving::_MultiInferenceRequest_default_instance_._instance,
  &::tensorflow::serving::_MultiInferenceResponse_default_instance_._instance,
};

const char descriptor_table_protodef_tensorflow_5fserving_2fapis_2finference_2eproto[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) =
  "\n\'tensorflow_serving/apis/inference.prot"
  "o\022\022tensorflow.serving\032,tensorflow_servin"
  "g/apis/classification.proto\032#tensorflow_"
  "serving/apis/input.proto\032#tensorflow_ser"
  "ving/apis/model.proto\032(tensorflow_servin"
  "g/apis/regression.proto\"W\n\rInferenceTask"
  "\0221\n\nmodel_spec\030\001 \001(\0132\035.tensorflow.servin"
  "g.ModelSpec\022\023\n\013method_name\030\002 \001(\t\"\334\001\n\017Inf"
  "erenceResult\0221\n\nmodel_spec\030\001 \001(\0132\035.tenso"
  "rflow.serving.ModelSpec\022I\n\025classificatio"
  "n_result\030\002 \001(\0132(.tensorflow.serving.Clas"
  "sificationResultH\000\022A\n\021regression_result\030"
  "\003 \001(\0132$.tensorflow.serving.RegressionRes"
  "ultH\000B\010\n\006result\"s\n\025MultiInferenceRequest"
  "\0220\n\005tasks\030\001 \003(\0132!.tensorflow.serving.Inf"
  "erenceTask\022(\n\005input\030\002 \001(\0132\031.tensorflow.s"
  "erving.Input\"N\n\026MultiInferenceResponse\0224"
  "\n\007results\030\001 \003(\0132#.tensorflow.serving.Inf"
  "erenceResultB\003\370\001\001b\006proto3"
  ;
static const ::_pbi::DescriptorTable* const descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_deps[4] = {
  &::descriptor_table_tensorflow_5fserving_2fapis_2fclassification_2eproto,
  &::descriptor_table_tensorflow_5fserving_2fapis_2finput_2eproto,
  &::descriptor_table_tensorflow_5fserving_2fapis_2fmodel_2eproto,
  &::descriptor_table_tensorflow_5fserving_2fapis_2fregression_2eproto,
};
static ::_pbi::once_flag descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_once;
const ::_pbi::DescriptorTable descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto = {
    false, false, 745, descriptor_table_protodef_tensorflow_5fserving_2fapis_2finference_2eproto,
    "tensorflow_serving/apis/inference.proto",
    &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_once, descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_deps, 4, 4,
    schemas, file_default_instances, TableStruct_tensorflow_5fserving_2fapis_2finference_2eproto::offsets,
    file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto, file_level_enum_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto,
    file_level_service_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto,
};
PROTOBUF_ATTRIBUTE_WEAK const ::_pbi::DescriptorTable* descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_getter() {
  return &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto;
}

// Force running AddDescriptors() at dynamic initialization time.
PROTOBUF_ATTRIBUTE_INIT_PRIORITY2 static ::_pbi::AddDescriptorsRunner dynamic_init_dummy_tensorflow_5fserving_2fapis_2finference_2eproto(&descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto);
namespace tensorflow {
namespace serving {

// ===================================================================

class InferenceTask::_Internal {
 public:
  static const ::tensorflow::serving::ModelSpec& model_spec(const InferenceTask* msg);
};

const ::tensorflow::serving::ModelSpec&
InferenceTask::_Internal::model_spec(const InferenceTask* msg) {
  return *msg->_impl_.model_spec_;
}
void InferenceTask::clear_model_spec() {
  if (GetArenaForAllocation() == nullptr && _impl_.model_spec_ != nullptr) {
    delete _impl_.model_spec_;
  }
  _impl_.model_spec_ = nullptr;
}
InferenceTask::InferenceTask(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.InferenceTask)
}
InferenceTask::InferenceTask(const InferenceTask& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  InferenceTask* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.method_name_){}
    , decltype(_impl_.model_spec_){nullptr}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  _impl_.method_name_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.method_name_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
  if (!from._internal_method_name().empty()) {
    _this->_impl_.method_name_.Set(from._internal_method_name(), 
      _this->GetArenaForAllocation());
  }
  if (from._internal_has_model_spec()) {
    _this->_impl_.model_spec_ = new ::tensorflow::serving::ModelSpec(*from._impl_.model_spec_);
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.InferenceTask)
}

inline void InferenceTask::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.method_name_){}
    , decltype(_impl_.model_spec_){nullptr}
    , /*decltype(_impl_._cached_size_)*/{}
  };
  _impl_.method_name_.InitDefault();
  #ifdef PROTOBUF_FORCE_COPY_DEFAULT_STRING
    _impl_.method_name_.Set("", GetArenaForAllocation());
  #endif // PROTOBUF_FORCE_COPY_DEFAULT_STRING
}

InferenceTask::~InferenceTask() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.InferenceTask)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void InferenceTask::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.method_name_.Destroy();
  if (this != internal_default_instance()) delete _impl_.model_spec_;
}

void InferenceTask::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void InferenceTask::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.InferenceTask)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.method_name_.ClearToEmpty();
  if (GetArenaForAllocation() == nullptr && _impl_.model_spec_ != nullptr) {
    delete _impl_.model_spec_;
  }
  _impl_.model_spec_ = nullptr;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* InferenceTask::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // .tensorflow.serving.ModelSpec model_spec = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr = ctx->ParseMessage(_internal_mutable_model_spec(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // string method_name = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          auto str = _internal_mutable_method_name();
          ptr = ::_pbi::InlineGreedyStringParser(str, ptr, ctx);
          CHK_(ptr);
          CHK_(::_pbi::VerifyUTF8(str, "tensorflow.serving.InferenceTask.method_name"));
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* InferenceTask::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.InferenceTask)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->_internal_has_model_spec()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(1, _Internal::model_spec(this),
        _Internal::model_spec(this).GetCachedSize(), target, stream);
  }

  // string method_name = 2;
  if (!this->_internal_method_name().empty()) {
    ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::VerifyUtf8String(
      this->_internal_method_name().data(), static_cast<int>(this->_internal_method_name().length()),
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::SERIALIZE,
      "tensorflow.serving.InferenceTask.method_name");
    target = stream->WriteStringMaybeAliased(
        2, this->_internal_method_name(), target);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.InferenceTask)
  return target;
}

size_t InferenceTask::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.InferenceTask)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // string method_name = 2;
  if (!this->_internal_method_name().empty()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::StringSize(
        this->_internal_method_name());
  }

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->_internal_has_model_spec()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.model_spec_);
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData InferenceTask::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    InferenceTask::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*InferenceTask::GetClassData() const { return &_class_data_; }


void InferenceTask::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<InferenceTask*>(&to_msg);
  auto& from = static_cast<const InferenceTask&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.InferenceTask)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (!from._internal_method_name().empty()) {
    _this->_internal_set_method_name(from._internal_method_name());
  }
  if (from._internal_has_model_spec()) {
    _this->_internal_mutable_model_spec()->::tensorflow::serving::ModelSpec::MergeFrom(
        from._internal_model_spec());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void InferenceTask::CopyFrom(const InferenceTask& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.InferenceTask)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool InferenceTask::IsInitialized() const {
  return true;
}

void InferenceTask::InternalSwap(InferenceTask* other) {
  using std::swap;
  auto* lhs_arena = GetArenaForAllocation();
  auto* rhs_arena = other->GetArenaForAllocation();
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  ::PROTOBUF_NAMESPACE_ID::internal::ArenaStringPtr::InternalSwap(
      &_impl_.method_name_, lhs_arena,
      &other->_impl_.method_name_, rhs_arena
  );
  swap(_impl_.model_spec_, other->_impl_.model_spec_);
}

::PROTOBUF_NAMESPACE_ID::Metadata InferenceTask::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_getter, &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_once,
      file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto[0]);
}

// ===================================================================

class InferenceResult::_Internal {
 public:
  static const ::tensorflow::serving::ModelSpec& model_spec(const InferenceResult* msg);
  static const ::tensorflow::serving::ClassificationResult& classification_result(const InferenceResult* msg);
  static const ::tensorflow::serving::RegressionResult& regression_result(const InferenceResult* msg);
};

const ::tensorflow::serving::ModelSpec&
InferenceResult::_Internal::model_spec(const InferenceResult* msg) {
  return *msg->_impl_.model_spec_;
}
const ::tensorflow::serving::ClassificationResult&
InferenceResult::_Internal::classification_result(const InferenceResult* msg) {
  return *msg->_impl_.result_.classification_result_;
}
const ::tensorflow::serving::RegressionResult&
InferenceResult::_Internal::regression_result(const InferenceResult* msg) {
  return *msg->_impl_.result_.regression_result_;
}
void InferenceResult::clear_model_spec() {
  if (GetArenaForAllocation() == nullptr && _impl_.model_spec_ != nullptr) {
    delete _impl_.model_spec_;
  }
  _impl_.model_spec_ = nullptr;
}
void InferenceResult::set_allocated_classification_result(::tensorflow::serving::ClassificationResult* classification_result) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  clear_result();
  if (classification_result) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(classification_result));
    if (message_arena != submessage_arena) {
      classification_result = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, classification_result, submessage_arena);
    }
    set_has_classification_result();
    _impl_.result_.classification_result_ = classification_result;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.classification_result)
}
void InferenceResult::clear_classification_result() {
  if (_internal_has_classification_result()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.result_.classification_result_;
    }
    clear_has_result();
  }
}
void InferenceResult::set_allocated_regression_result(::tensorflow::serving::RegressionResult* regression_result) {
  ::PROTOBUF_NAMESPACE_ID::Arena* message_arena = GetArenaForAllocation();
  clear_result();
  if (regression_result) {
    ::PROTOBUF_NAMESPACE_ID::Arena* submessage_arena =
        ::PROTOBUF_NAMESPACE_ID::Arena::InternalGetOwningArena(
                reinterpret_cast<::PROTOBUF_NAMESPACE_ID::MessageLite*>(regression_result));
    if (message_arena != submessage_arena) {
      regression_result = ::PROTOBUF_NAMESPACE_ID::internal::GetOwnedMessage(
          message_arena, regression_result, submessage_arena);
    }
    set_has_regression_result();
    _impl_.result_.regression_result_ = regression_result;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.regression_result)
}
void InferenceResult::clear_regression_result() {
  if (_internal_has_regression_result()) {
    if (GetArenaForAllocation() == nullptr) {
      delete _impl_.result_.regression_result_;
    }
    clear_has_result();
  }
}
InferenceResult::InferenceResult(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.InferenceResult)
}
InferenceResult::InferenceResult(const InferenceResult& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  InferenceResult* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.model_spec_){nullptr}
    , decltype(_impl_.result_){}
    , /*decltype(_impl_._cached_size_)*/{}
    , /*decltype(_impl_._oneof_case_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_model_spec()) {
    _this->_impl_.model_spec_ = new ::tensorflow::serving::ModelSpec(*from._impl_.model_spec_);
  }
  clear_has_result();
  switch (from.result_case()) {
    case kClassificationResult: {
      _this->_internal_mutable_classification_result()->::tensorflow::serving::ClassificationResult::MergeFrom(
          from._internal_classification_result());
      break;
    }
    case kRegressionResult: {
      _this->_internal_mutable_regression_result()->::tensorflow::serving::RegressionResult::MergeFrom(
          from._internal_regression_result());
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.InferenceResult)
}

inline void InferenceResult::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.model_spec_){nullptr}
    , decltype(_impl_.result_){}
    , /*decltype(_impl_._cached_size_)*/{}
    , /*decltype(_impl_._oneof_case_)*/{}
  };
  clear_has_result();
}

InferenceResult::~InferenceResult() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.InferenceResult)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void InferenceResult::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  if (this != internal_default_instance()) delete _impl_.model_spec_;
  if (has_result()) {
    clear_result();
  }
}

void InferenceResult::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void InferenceResult::clear_result() {
// @@protoc_insertion_point(one_of_clear_start:tensorflow.serving.InferenceResult)
  switch (result_case()) {
    case kClassificationResult: {
      if (GetArenaForAllocation() == nullptr) {
        delete _impl_.result_.classification_result_;
      }
      break;
    }
    case kRegressionResult: {
      if (GetArenaForAllocation() == nullptr) {
        delete _impl_.result_.regression_result_;
      }
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
  _impl_._oneof_case_[0] = RESULT_NOT_SET;
}


void InferenceResult::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.InferenceResult)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  if (GetArenaForAllocation() == nullptr && _impl_.model_spec_ != nullptr) {
    delete _impl_.model_spec_;
  }
  _impl_.model_spec_ = nullptr;
  clear_result();
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* InferenceResult::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // .tensorflow.serving.ModelSpec model_spec = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr = ctx->ParseMessage(_internal_mutable_model_spec(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.serving.ClassificationResult classification_result = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          ptr = ctx->ParseMessage(_internal_mutable_classification_result(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.serving.RegressionResult regression_result = 3;
      case 3:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 26)) {
          ptr = ctx->ParseMessage(_internal_mutable_regression_result(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* InferenceResult::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.InferenceResult)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->_internal_has_model_spec()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(1, _Internal::model_spec(this),
        _Internal::model_spec(this).GetCachedSize(), target, stream);
  }

  // .tensorflow.serving.ClassificationResult classification_result = 2;
  if (_internal_has_classification_result()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(2, _Internal::classification_result(this),
        _Internal::classification_result(this).GetCachedSize(), target, stream);
  }

  // .tensorflow.serving.RegressionResult regression_result = 3;
  if (_internal_has_regression_result()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(3, _Internal::regression_result(this),
        _Internal::regression_result(this).GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.InferenceResult)
  return target;
}

size_t InferenceResult::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.InferenceResult)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->_internal_has_model_spec()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.model_spec_);
  }

  switch (result_case()) {
    // .tensorflow.serving.ClassificationResult classification_result = 2;
    case kClassificationResult: {
      total_size += 1 +
        ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
          *_impl_.result_.classification_result_);
      break;
    }
    // .tensorflow.serving.RegressionResult regression_result = 3;
    case kRegressionResult: {
      total_size += 1 +
        ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
          *_impl_.result_.regression_result_);
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData InferenceResult::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    InferenceResult::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*InferenceResult::GetClassData() const { return &_class_data_; }


void InferenceResult::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<InferenceResult*>(&to_msg);
  auto& from = static_cast<const InferenceResult&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.InferenceResult)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (from._internal_has_model_spec()) {
    _this->_internal_mutable_model_spec()->::tensorflow::serving::ModelSpec::MergeFrom(
        from._internal_model_spec());
  }
  switch (from.result_case()) {
    case kClassificationResult: {
      _this->_internal_mutable_classification_result()->::tensorflow::serving::ClassificationResult::MergeFrom(
          from._internal_classification_result());
      break;
    }
    case kRegressionResult: {
      _this->_internal_mutable_regression_result()->::tensorflow::serving::RegressionResult::MergeFrom(
          from._internal_regression_result());
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void InferenceResult::CopyFrom(const InferenceResult& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.InferenceResult)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool InferenceResult::IsInitialized() const {
  return true;
}

void InferenceResult::InternalSwap(InferenceResult* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  swap(_impl_.model_spec_, other->_impl_.model_spec_);
  swap(_impl_.result_, other->_impl_.result_);
  swap(_impl_._oneof_case_[0], other->_impl_._oneof_case_[0]);
}

::PROTOBUF_NAMESPACE_ID::Metadata InferenceResult::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_getter, &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_once,
      file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto[1]);
}

// ===================================================================

class MultiInferenceRequest::_Internal {
 public:
  static const ::tensorflow::serving::Input& input(const MultiInferenceRequest* msg);
};

const ::tensorflow::serving::Input&
MultiInferenceRequest::_Internal::input(const MultiInferenceRequest* msg) {
  return *msg->_impl_.input_;
}
void MultiInferenceRequest::clear_input() {
  if (GetArenaForAllocation() == nullptr && _impl_.input_ != nullptr) {
    delete _impl_.input_;
  }
  _impl_.input_ = nullptr;
}
MultiInferenceRequest::MultiInferenceRequest(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.MultiInferenceRequest)
}
MultiInferenceRequest::MultiInferenceRequest(const MultiInferenceRequest& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  MultiInferenceRequest* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.tasks_){from._impl_.tasks_}
    , decltype(_impl_.input_){nullptr}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  if (from._internal_has_input()) {
    _this->_impl_.input_ = new ::tensorflow::serving::Input(*from._impl_.input_);
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.MultiInferenceRequest)
}

inline void MultiInferenceRequest::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.tasks_){arena}
    , decltype(_impl_.input_){nullptr}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

MultiInferenceRequest::~MultiInferenceRequest() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.MultiInferenceRequest)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void MultiInferenceRequest::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.tasks_.~RepeatedPtrField();
  if (this != internal_default_instance()) delete _impl_.input_;
}

void MultiInferenceRequest::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void MultiInferenceRequest::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.MultiInferenceRequest)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.tasks_.Clear();
  if (GetArenaForAllocation() == nullptr && _impl_.input_ != nullptr) {
    delete _impl_.input_;
  }
  _impl_.input_ = nullptr;
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* MultiInferenceRequest::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // repeated .tensorflow.serving.InferenceTask tasks = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_tasks(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<10>(ptr));
        } else
          goto handle_unusual;
        continue;
      // .tensorflow.serving.Input input = 2;
      case 2:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 18)) {
          ptr = ctx->ParseMessage(_internal_mutable_input(), ptr);
          CHK_(ptr);
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* MultiInferenceRequest::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.MultiInferenceRequest)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .tensorflow.serving.InferenceTask tasks = 1;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_tasks_size()); i < n; i++) {
    const auto& repfield = this->_internal_tasks(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(1, repfield, repfield.GetCachedSize(), target, stream);
  }

  // .tensorflow.serving.Input input = 2;
  if (this->_internal_has_input()) {
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
      InternalWriteMessage(2, _Internal::input(this),
        _Internal::input(this).GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.MultiInferenceRequest)
  return target;
}

size_t MultiInferenceRequest::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.MultiInferenceRequest)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated .tensorflow.serving.InferenceTask tasks = 1;
  total_size += 1UL * this->_internal_tasks_size();
  for (const auto& msg : this->_impl_.tasks_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  // .tensorflow.serving.Input input = 2;
  if (this->_internal_has_input()) {
    total_size += 1 +
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(
        *_impl_.input_);
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData MultiInferenceRequest::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    MultiInferenceRequest::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*MultiInferenceRequest::GetClassData() const { return &_class_data_; }


void MultiInferenceRequest::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<MultiInferenceRequest*>(&to_msg);
  auto& from = static_cast<const MultiInferenceRequest&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.MultiInferenceRequest)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.tasks_.MergeFrom(from._impl_.tasks_);
  if (from._internal_has_input()) {
    _this->_internal_mutable_input()->::tensorflow::serving::Input::MergeFrom(
        from._internal_input());
  }
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void MultiInferenceRequest::CopyFrom(const MultiInferenceRequest& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.MultiInferenceRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiInferenceRequest::IsInitialized() const {
  return true;
}

void MultiInferenceRequest::InternalSwap(MultiInferenceRequest* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.tasks_.InternalSwap(&other->_impl_.tasks_);
  swap(_impl_.input_, other->_impl_.input_);
}

::PROTOBUF_NAMESPACE_ID::Metadata MultiInferenceRequest::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_getter, &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_once,
      file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto[2]);
}

// ===================================================================

class MultiInferenceResponse::_Internal {
 public:
};

MultiInferenceResponse::MultiInferenceResponse(::PROTOBUF_NAMESPACE_ID::Arena* arena,
                         bool is_message_owned)
  : ::PROTOBUF_NAMESPACE_ID::Message(arena, is_message_owned) {
  SharedCtor(arena, is_message_owned);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.MultiInferenceResponse)
}
MultiInferenceResponse::MultiInferenceResponse(const MultiInferenceResponse& from)
  : ::PROTOBUF_NAMESPACE_ID::Message() {
  MultiInferenceResponse* const _this = this; (void)_this;
  new (&_impl_) Impl_{
      decltype(_impl_.results_){from._impl_.results_}
    , /*decltype(_impl_._cached_size_)*/{}};

  _internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.MultiInferenceResponse)
}

inline void MultiInferenceResponse::SharedCtor(
    ::_pb::Arena* arena, bool is_message_owned) {
  (void)arena;
  (void)is_message_owned;
  new (&_impl_) Impl_{
      decltype(_impl_.results_){arena}
    , /*decltype(_impl_._cached_size_)*/{}
  };
}

MultiInferenceResponse::~MultiInferenceResponse() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.MultiInferenceResponse)
  if (auto *arena = _internal_metadata_.DeleteReturnArena<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>()) {
  (void)arena;
    return;
  }
  SharedDtor();
}

inline void MultiInferenceResponse::SharedDtor() {
  GOOGLE_DCHECK(GetArenaForAllocation() == nullptr);
  _impl_.results_.~RepeatedPtrField();
}

void MultiInferenceResponse::SetCachedSize(int size) const {
  _impl_._cached_size_.Set(size);
}

void MultiInferenceResponse::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.MultiInferenceResponse)
  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.results_.Clear();
  _internal_metadata_.Clear<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>();
}

const char* MultiInferenceResponse::_InternalParse(const char* ptr, ::_pbi::ParseContext* ctx) {
#define CHK_(x) if (PROTOBUF_PREDICT_FALSE(!(x))) goto failure
  while (!ctx->Done(&ptr)) {
    uint32_t tag;
    ptr = ::_pbi::ReadTag(ptr, &tag);
    switch (tag >> 3) {
      // repeated .tensorflow.serving.InferenceResult results = 1;
      case 1:
        if (PROTOBUF_PREDICT_TRUE(static_cast<uint8_t>(tag) == 10)) {
          ptr -= 1;
          do {
            ptr += 1;
            ptr = ctx->ParseMessage(_internal_add_results(), ptr);
            CHK_(ptr);
            if (!ctx->DataAvailable(ptr)) break;
          } while (::PROTOBUF_NAMESPACE_ID::internal::ExpectTag<10>(ptr));
        } else
          goto handle_unusual;
        continue;
      default:
        goto handle_unusual;
    }  // switch
  handle_unusual:
    if ((tag == 0) || ((tag & 7) == 4)) {
      CHK_(ptr);
      ctx->SetLastTag(tag);
      goto message_done;
    }
    ptr = UnknownFieldParse(
        tag,
        _internal_metadata_.mutable_unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(),
        ptr, ctx);
    CHK_(ptr != nullptr);
  }  // while
message_done:
  return ptr;
failure:
  ptr = nullptr;
  goto message_done;
#undef CHK_
}

uint8_t* MultiInferenceResponse::_InternalSerialize(
    uint8_t* target, ::PROTOBUF_NAMESPACE_ID::io::EpsCopyOutputStream* stream) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.MultiInferenceResponse)
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .tensorflow.serving.InferenceResult results = 1;
  for (unsigned i = 0,
      n = static_cast<unsigned>(this->_internal_results_size()); i < n; i++) {
    const auto& repfield = this->_internal_results(i);
    target = ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::
        InternalWriteMessage(1, repfield, repfield.GetCachedSize(), target, stream);
  }

  if (PROTOBUF_PREDICT_FALSE(_internal_metadata_.have_unknown_fields())) {
    target = ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(::PROTOBUF_NAMESPACE_ID::UnknownFieldSet::default_instance), target, stream);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.MultiInferenceResponse)
  return target;
}

size_t MultiInferenceResponse::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.MultiInferenceResponse)
  size_t total_size = 0;

  uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated .tensorflow.serving.InferenceResult results = 1;
  total_size += 1UL * this->_internal_results_size();
  for (const auto& msg : this->_impl_.results_) {
    total_size +=
      ::PROTOBUF_NAMESPACE_ID::internal::WireFormatLite::MessageSize(msg);
  }

  return MaybeComputeUnknownFieldsSize(total_size, &_impl_._cached_size_);
}

const ::PROTOBUF_NAMESPACE_ID::Message::ClassData MultiInferenceResponse::_class_data_ = {
    ::PROTOBUF_NAMESPACE_ID::Message::CopyWithSourceCheck,
    MultiInferenceResponse::MergeImpl
};
const ::PROTOBUF_NAMESPACE_ID::Message::ClassData*MultiInferenceResponse::GetClassData() const { return &_class_data_; }


void MultiInferenceResponse::MergeImpl(::PROTOBUF_NAMESPACE_ID::Message& to_msg, const ::PROTOBUF_NAMESPACE_ID::Message& from_msg) {
  auto* const _this = static_cast<MultiInferenceResponse*>(&to_msg);
  auto& from = static_cast<const MultiInferenceResponse&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.MultiInferenceResponse)
  GOOGLE_DCHECK_NE(&from, _this);
  uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_impl_.results_.MergeFrom(from._impl_.results_);
  _this->_internal_metadata_.MergeFrom<::PROTOBUF_NAMESPACE_ID::UnknownFieldSet>(from._internal_metadata_);
}

void MultiInferenceResponse::CopyFrom(const MultiInferenceResponse& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.MultiInferenceResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiInferenceResponse::IsInitialized() const {
  return true;
}

void MultiInferenceResponse::InternalSwap(MultiInferenceResponse* other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.results_.InternalSwap(&other->_impl_.results_);
}

::PROTOBUF_NAMESPACE_ID::Metadata MultiInferenceResponse::GetMetadata() const {
  return ::_pbi::AssignDescriptors(
      &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_getter, &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_once,
      file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto[3]);
}

// @@protoc_insertion_point(namespace_scope)
}  // namespace serving
}  // namespace tensorflow
PROTOBUF_NAMESPACE_OPEN
template<> PROTOBUF_NOINLINE ::tensorflow::serving::InferenceTask*
Arena::CreateMaybeMessage< ::tensorflow::serving::InferenceTask >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::InferenceTask >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::serving::InferenceResult*
Arena::CreateMaybeMessage< ::tensorflow::serving::InferenceResult >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::InferenceResult >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::serving::MultiInferenceRequest*
Arena::CreateMaybeMessage< ::tensorflow::serving::MultiInferenceRequest >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::MultiInferenceRequest >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::serving::MultiInferenceResponse*
Arena::CreateMaybeMessage< ::tensorflow::serving::MultiInferenceResponse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::MultiInferenceResponse >(arena);
}
PROTOBUF_NAMESPACE_CLOSE

// @@protoc_insertion_point(global_scope)
#include <google/protobuf/port_undef.inc>
